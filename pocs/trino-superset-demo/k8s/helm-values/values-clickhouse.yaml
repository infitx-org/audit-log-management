global:
  defaultStorageClass: ""
  storageClass: ""

image:
  registry: docker.io
  repository: bitnami/clickhouse
  tag: 25.3.2-debian-12-r3
  debug: false

resourcesPreset: "large"
## @param shards Number of ClickHouse shards to deploy
##
shards: 1
## @param replicaCount Number of ClickHouse replicas per shard to deploy
## if keeper enable, same as keeper count, keeper cluster by shards.
##
replicaCount: 1
## @param distributeReplicasByZone Schedules replicas of the same shard to different availability zones
##
distributeReplicasByZone: false
## @param containerPorts.http ClickHouse HTTP container port
## @param containerPorts.https ClickHouse HTTPS container port
## @param containerPorts.tcp ClickHouse TCP container port
## @param containerPorts.tcpSecure ClickHouse TCP (secure) container port
## @param containerPorts.keeper ClickHouse keeper TCP container port
## @param containerPorts.keeperSecure ClickHouse keeper TCP (secure) container port
## @param containerPorts.keeperInter ClickHouse keeper interserver TCP container port
## @param containerPorts.mysql ClickHouse MySQL container port
## @param containerPorts.postgresql ClickHouse PostgreSQL container port
## @param containerPorts.interserver ClickHouse Interserver container port
## @param containerPorts.metrics ClickHouse metrics container port
##
containerPorts:
  http: 8123
  https: 8443
  tcp: 9000
  tcpSecure: 9440
  keeper: 2181
  keeperSecure: 3181
  keeperInter: 9444
  mysql: 9004
  postgresql: 9005
  interserver: 9009
  metrics: 8001

## @param initdbScripts Dictionary of initdb scripts
## Specify dictionary of scripts to be run at first boot
initdbScripts:
  my_init_script.sh: |
    #!/bin/bash
    echo "Creating table..."
    clickhouse-client --user=default --password=supersecretpassword --query "
      CREATE TABLE audit_logs
      (
          trace_id String,
          span_id String,
          trace_service String,
          event_id String,
          event_type String,
          event_action String,
          event_status String,
          source String,
          destination String,
          transaction_type String,
          transaction_action String,
          audit_type String,
          content_type String,
          service_name String,
          operation String,
          http_method String,
          http_path String,
          http_query String,
          http_url String,
          party_id_type String,
          party_identifier String,
          party_sub_id_or_type String,
          request_id String,
          oracle_id String,
          quote_id String,
          transaction_id String,
          conversion_request_id String,
          conversion_id String,
          determining_transfer_id String,
          transfer_id String,
          commit_request_id String,
          timestamp UInt64,
          line String,
          year UInt16,
          month UInt8,
          day UInt8,
          hour UInt8
      )
      ENGINE = S3(
          'http://minio:9000/audit-env1/orc/year=*/month=*/day=*/hour=*/*.orc',
          'admin',
          'admin123',
          'ORC'
      )
      SETTINGS 
          input_format_with_names_use_header = 1,
          input_format_allow_errors_num = 10;
    "
    echo "Table created successfully."
## Authentication
## @param auth.username ClickHouse Admin username
## @param auth.password ClickHouse Admin password
## @param auth.existingSecret Name of a secret containing the Admin password
## @param auth.existingSecretKey Name of the key inside the existing secret
##
auth:
  username: default
  password: "supersecretpassword"
  existingSecret: ""
  existingSecretKey: ""
## @param logLevel Logging level
##
logLevel: information
## @section ClickHouse keeper configuration parameters
## @param keeper.enabled Deploy ClickHouse keeper. Support is experimental.
##
keeper:
  enabled: false


## ClickHouse service parameters
##
service:
  ## @param service.type ClickHouse service type
  ##
  type: ClusterIP
  ## @param service.ports.http ClickHouse service HTTP port
  ## @param service.ports.https ClickHouse service HTTPS port
  ## @param service.ports.tcp ClickHouse service TCP port
  ## @param service.ports.tcpSecure ClickHouse service TCP (secure) port
  ## @param service.ports.keeper ClickHouse keeper TCP container port
  ## @param service.ports.keeperSecure ClickHouse keeper TCP (secure) container port
  ## @param service.ports.keeperInter ClickHouse keeper interserver TCP container port
  ## @param service.ports.mysql ClickHouse service MySQL port
  ## @param service.ports.postgresql ClickHouse service PostgreSQL port
  ## @param service.ports.interserver ClickHouse service Interserver port
  ## @param service.ports.metrics ClickHouse service metrics port
  ##
  ports:
    http: 8123
    https: 443
    tcp: 9000
    tcpSecure: 9440
    keeper: 2181
    keeperSecure: 3181
    keeperInter: 9444
    mysql: 9004
    postgresql: 9005
    interserver: 9009
    metrics: 8001
  ## Node ports to expose
  ## @param service.nodePorts.http Node port for HTTP
  ## @param service.nodePorts.https Node port for HTTPS
  ## @param service.nodePorts.tcp Node port for TCP
  ## @param service.nodePorts.tcpSecure Node port for TCP (with TLS)
  ## @param service.nodePorts.keeper ClickHouse keeper TCP container port
  ## @param service.nodePorts.keeperSecure ClickHouse keeper TCP (secure) container port
  ## @param service.nodePorts.keeperInter ClickHouse keeper interserver TCP container port
  ## @param service.nodePorts.mysql Node port for MySQL
  ## @param service.nodePorts.postgresql Node port for PostgreSQL
  ## @param service.nodePorts.interserver Node port for Interserver
  ## @param service.nodePorts.metrics Node port for metrics
  ## NOTE: choose port between <30000-32767>
  ##
  nodePorts:
    http: ""
    https: ""
    tcp: ""
    tcpSecure: ""
    keeper: ""
    keeperSecure: ""
    keeperInter: ""
    mysql: ""
    postgresql: ""
    interserver: ""
    metrics: ""
  ## @param service.clusterIP ClickHouse service Cluster IP
  ## e.g.:
  ## clusterIP: None
  ##
  clusterIP: ""
  ## @param service.loadBalancerIP ClickHouse service Load Balancer IP
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
  ##
  loadBalancerIP: ""
  ## @param service.loadBalancerSourceRanges ClickHouse service Load Balancer sources
  ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
  ## e.g:
  ## loadBalancerSourceRanges:
  ##   - 10.10.10.0/24
  ##
  loadBalancerSourceRanges: []
  ## @param service.externalTrafficPolicy ClickHouse service external traffic policy
  ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
  ##
  externalTrafficPolicy: Cluster
  ## @param service.annotations Additional custom annotations for ClickHouse service
  ##
  annotations: {}
  ## @param service.extraPorts Extra ports to expose in ClickHouse service (normally used with the `sidecars` value)
  ##
  extraPorts: []
  ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
  ## Values: ClientIP or None
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
  ##
  sessionAffinity: None
  ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
  ## sessionAffinityConfig:
  ##   clientIP:
  ##     timeoutSeconds: 300
  ##
  sessionAffinityConfig: {}
  ## Headless service properties
  ##
  headless:
    ## @param service.headless.annotations Annotations for the headless service.
    ##
    annotations: {}


## Enable persistence using Persistent Volume Claims
## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
##
persistence:
  ## @param persistence.enabled Enable persistence using Persistent Volume Claims
  ##
  enabled: false
  ## @param persistence.existingClaim Name of an existing PVC to use
  ##
  existingClaim: ""
  ## @param persistence.storageClass Storage class of backing PVC
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: ""
  ## @param persistence.labels Persistent Volume Claim labels
  ##
  labels: {}
  ## @param persistence.annotations Persistent Volume Claim annotations
  ##
  annotations: {}
  ## @param persistence.accessModes Persistent Volume Access Modes
  ##
  accessModes:
    - ReadWriteOnce
  ## @param persistence.size Size of data volume
  ##
  size: 8Gi
  ## @param persistence.selector Selector to match an existing Persistent Volume for ClickHouse data PVC
  ## If set, the PVC can't have a PV dynamically provisioned for it
  ## E.g.
  ## selector:
  ##   matchLabels:
  ##     app: my-app
  ##
  selector: {}
  ## @param persistence.dataSource Custom PVC data source
  ##
  dataSource: {}


## Prometheus metrics
##
metrics:
  ## @param metrics.enabled Enable the export of Prometheus metrics
  ##
  enabled: false
  ## @param metrics.podAnnotations [object] Annotations for metrics scraping
  ##
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "{{ .Values.containerPorts.metrics }}"
  ## Prometheus Operator ServiceMonitor configuration
  ##
  serviceMonitor:
    ## @param metrics.serviceMonitor.enabled if `true`, creates a Prometheus Operator ServiceMonitor (also requires `metrics.enabled` to be `true`)
    ##
    enabled: false
    ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running
    ##
    namespace: ""
    ## @param metrics.serviceMonitor.annotations Additional custom annotations for the ServiceMonitor
    ##
    annotations: {}
    ## @param metrics.serviceMonitor.labels Extra labels for the ServiceMonitor
    ##
    labels: {}
    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in Prometheus
    ##
    jobLabel: ""
    ## @param metrics.serviceMonitor.honorLabels honorLabels chooses the metric's labels on collisions with target labels
    ##
    honorLabels: false
    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped.
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ## e.g:
    ## interval: 10s
    ##
    interval: ""
    ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ## e.g:
    ## scrapeTimeout: 10s
    ##
    scrapeTimeout: ""
    ## @param metrics.serviceMonitor.metricRelabelings Specify additional relabeling of metrics
    ##
    metricRelabelings: []
    ## @param metrics.serviceMonitor.relabelings Specify general relabeling
    ##
    relabelings: []
    ## @param metrics.serviceMonitor.selector Prometheus instance selector labels
    ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
    ## selector:
    ##   prometheus: my-prometheus
    ##
    selector: {}
  ## Prometheus Operator PrometheusRule configuration
  ##
  prometheusRule:
    ## @param metrics.prometheusRule.enabled Create a PrometheusRule for Prometheus Operator
    ##
    enabled: false
    ## @param metrics.prometheusRule.namespace Namespace for the PrometheusRule Resource (defaults to the Release Namespace)
    ##
    namespace: ""
    ## @param metrics.prometheusRule.additionalLabels Additional labels that can be used so PrometheusRule will be discovered by Prometheus
    ##
    additionalLabels: {}
    ## @param metrics.prometheusRule.rules PrometheusRule definitions
    ##  - alert: ClickhouseServerRestart
    ##    annotations:
    ##      message: Clickhouse-server started recently
    ##    expr: ClickHouseAsyncMetrics_Uptime > 1 < 180
    ##    for: 5m
    ##    labels:
    ##      severity: warning
    rules: []
## @section Zookeeper subchart parameters
##
## @param zookeeper.enabled Deploy Zookeeper subchart
## @param zookeeper.replicaCount Number of Zookeeper instances
## @param zookeeper.service.ports.client Zookeeper client port
##
zookeeper:
  enabled: true
  replicaCount: 1
  persistence:
    enabled: false
  service:
    ports:
      client: 2181
  ## ZooKeeper resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param zookeeper.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param zookeeper.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
